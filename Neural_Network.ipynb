{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4cFgDoQDAtHWaaxMHofn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkMburu/Heart-Disease-Classification-deepLearning-pytorch/blob/master/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUO3_0jF52M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmO100LDGAFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Linear Regression\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, n_classes):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffwsu6DSGOHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Logistic Regression\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, n_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        # x = F.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH_O4nGQGinz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Neural network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
        "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "        self.fc3 = nn.Linear(hidden_size[1], n_classes)\n",
        "\n",
        "        self.drop_layer = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = self.drop_layer(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = self.drop_layer(x)\n",
        "        x = self.fc3(x)\n",
        "        # x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ_MeK_lGzWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Classification Accuracy\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the classification accuracy over the k top predictions for the\n",
        "    specified values of k\"\"\"\n",
        "\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOiqXtZ3G6J0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Confusion Matrix\n",
        "class EvalMetric(object):\n",
        "    \"\"\"A class contains different metrics that can be used for evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, target, pred, num_classes, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target: target labels\n",
        "            pred: model predictions\n",
        "            num_classes (int): number of classes\n",
        "            verbose (bool): display detailed results (Defualt=False)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.target = target\n",
        "        self.pred = pred\n",
        "        self.num_classes = num_classes\n",
        "        self.verbose = verbose\n",
        "        self.conf_matrix = self.ConfusionMatrix\n",
        "        self.true_false_pos_neg = self.TrueFalseCondition\n",
        "        if self.verbose:\n",
        "            print('Number of classes {} - Size of target labels {} - '\n",
        "                  'Size of predicted labels = {}'.format(self.num_classes,\n",
        "                                                         self.target.size,\n",
        "                                                         self.pred.size))\n",
        "\n",
        "    # =========================================================================\n",
        "    # Confusion Matrix\n",
        "    @property\n",
        "    def ConfusionMatrix(self):\n",
        "        \"\"\"Computes the confusion matrix\"\"\"\n",
        "        conf_matrix = torch.zeros([self.num_classes, self.num_classes], dtype=torch.int32)\n",
        "        for t, p in zip(self.target, self.pred):\n",
        "            conf_matrix[t, p] += 1\n",
        "        if self.verbose:\n",
        "            print('Confusion matrix\\n', conf_matrix)\n",
        "\n",
        "        return conf_matrix\n",
        "\n",
        "    # =========================================================================\n",
        "    # True/False, Postive/Negative\n",
        "    @property\n",
        "    def TrueFalseCondition(self):\n",
        "        \"\"\"Computes true VS predicted conditon values:\n",
        "        Retrurns:\n",
        "            [True Positive (TP), True Negative (TN),\n",
        "            False Positive (FP), False Negative (FN)]\n",
        "        \"\"\"\n",
        "        conf_matrix = self.conf_matrix\n",
        "        TP = conf_matrix.diag()\n",
        "        TN = 0 * TP\n",
        "        FP = 0 * TP\n",
        "        FN = 0 * TP\n",
        "        for c in range(self.num_classes):\n",
        "            idx = torch.ones(self.num_classes).byte()\n",
        "            idx[c] = 0\n",
        "            # all non-class samples classified as non-class\n",
        "            TN[c] = conf_matrix[idx.nonzero()[:,\n",
        "                                None], idx.nonzero()].sum()  # conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
        "            # all non-class samples classified as class\n",
        "            FP[c] = conf_matrix[idx, c].sum()\n",
        "            # all class samples not classified as class\n",
        "            FN[c] = conf_matrix[c, idx].sum()\n",
        "            if self.verbose:\n",
        "                print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
        "                    c, TP[c], TN[c], FP[c], FN[c]))\n",
        "\n",
        "        return TP, TN, FP, FN\n",
        "\n",
        "    def ClassificationAccuracy(self):\n",
        "        \"\"\"Compute classification rate or accuracy:\n",
        "            (TP+TN)/(TP+TN+FP+FN)\n",
        "        \"\"\"\n",
        "        TFPN = self.true_false_pos_neg\n",
        "        return (TFPN[0] + TFPN[1]) / TFPN.sum()\n",
        "\n",
        "    def precision(self):\n",
        "        \"\"\"Compute precision:\n",
        "            TP / (TP + FP)\n",
        "        \"\"\"\n",
        "        TFPN = self.true_false_pos_neg\n",
        "        return TFPN[0] / (TFPN[0]+TFPN[2])\n",
        "\n",
        "    def sensitivity(self):\n",
        "        \"\"\"Compute sensitivity (recall):\n",
        "           TP / (TP + FN)\n",
        "        \"\"\"\n",
        "        TFPN = self.true_false_pos_neg\n",
        "        return TFPN[0] / (TFPN[0]+TFPN[3])\n",
        "\n",
        "    def f1score(self):\n",
        "        \"\"\"Compute sensitivity (recall):\n",
        "            (2 * (Precision * Recall))/(Precision + Recall)\n",
        "        \"\"\"\n",
        "        return 2.0 * (self.precision * self.sensitivity)/(self.precision + self.sensitivity)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHl9HdKTD_oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7KxH8lHHmrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_acc1 = 0\n",
        "model_names = ['NeuralNet', 'LogisticRegression']\n",
        "LABELS = ['0', '1', '2', '3', '4']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeUpJORUIlyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset module to prepare data for the learning pipeline\n",
        "def count_samples_per_class(labels):\n",
        "    \"\"\"\n",
        "    Count number of samples per class\n",
        "    :param labels:\n",
        "    :return: vector of size(labels) containing each class frequency\n",
        "    \"\"\"\n",
        "    return torch.FloatTensor([len(np.where(labels == t)[0]) for t in np.unique(labels)])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93eDNR3wI602",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def assign_sample_weight(labels, weights):\n",
        "    \"\"\"\n",
        "    Assign weight for each sample\n",
        "    :param labels:\n",
        "    :param weights:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return torch.FloatTensor([weights[t] for t in labels])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgthWuQkJBPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_class_weight(n_samples, n_classes, class_bincount):\n",
        "    \"\"\"\n",
        "    Estimate class weights for unbalanced datasets.\n",
        "    Class weights are calculated by: n_samples / (n_classes * class_sample_count)\n",
        "    :param n_samples:\n",
        "    :param n_classes:\n",
        "    :param class_bincount:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return torch.FloatTensor(n_samples / (n_classes * class_bincount))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDI7kZMVJHNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HeartDiseaseDataset(Dataset):\n",
        "    \"\"\"A dataset class to retrieve samples of paired images and labels\"\"\"\n",
        "\n",
        "    def __init__(self, csv, shuffle=None, label_names=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv (string): Path to the csv file with data\n",
        "            shuffle (callable, optional): Shuffle list of files\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # self.transform = transform\n",
        "        self.csv_file = pd.read_csv(csv)\n",
        "        self.label_names = label_names\n",
        "\n",
        "        labels = self.csv_file['num']\n",
        "\n",
        "        self.class_sample_count = count_samples_per_class(labels)\n",
        "        self.class_probability = self.class_sample_count / len(labels)\n",
        "        self.sample_weights = assign_sample_weight(labels, 1. / self.class_sample_count)\n",
        "        self.class_weights = compute_class_weight(n_samples=self.__len__(),\n",
        "                                                  n_classes=len(self.class_sample_count),\n",
        "                                                  class_bincount=self.class_sample_count)\n",
        "\n",
        "        if shuffle:\n",
        "            self.csv_file = self.csv_file.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        features = torch.tensor(self.csv_file.iloc[:, :13].values[idx]).float()\n",
        "        target = torch.tensor(self.csv_file['num'].values[idx])\n",
        "\n",
        "        return {'features': features, 'target': target}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V94FbCSJQmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac9cfa7a-71ed-451e-ea5a-a8d0b7553f20"
      },
      "source": [
        "####Building Neural Network\n",
        "parser = argparse.ArgumentParser(description='Heart Disease Diagnosis')\n",
        "parser.add_argument('-s', '--save_dir', metavar='SAVE_DIR',\n",
        "                    help='path to the save directory', default='models')\n",
        "parser.add_argument('-t', '--train-file', metavar='TRAIN_FILE', default=None,\n",
        "                    help='path to the csv file that contain train images')\n",
        "parser.add_argument('-v', '--valid-file', metavar='VALID_FILE', default=None,\n",
        "                    help='path to the csv file that contain validation data')\n",
        "parser.add_argument('-c', '--classes', default=5, type=int,\n",
        "                    metavar='CLASSES', help='number of classes')\n",
        "parser.add_argument('-a', '--arch', metavar='ARCH', default='NeuralNet',\n",
        "                    choices=model_names,\n",
        "                    help='model architecture: ' + ' | '.join(model_names) + ' (default: NeuralNet)')\n",
        "parser.add_argument('-j', '--workers', default=os.cpu_count(),\n",
        "                    type=int, metavar='N',\n",
        "                    help='number of data loading workers (default: max)')\n",
        "parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('-b', '--batch-size', default=500, type=int,\n",
        "                    metavar='N',\n",
        "                    help='mini-batch size (default: 100), this is the total')\n",
        "parser.add_argument('--optim', default='adam', type=str, metavar='OPTIM',\n",
        "                    help='select optimizer [sgd, adam]',\n",
        "                    dest='optim')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
        "                    metavar='LR', help='initial learning rate', dest='lr')\n",
        "parser.add_argument('--lr_scheduler', default=None, type=str, metavar='LR_SCH',\n",
        "                    help='learning scheduler [reduce, cyclic, cosine]',\n",
        "                    dest='lr_scheduler')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                    help='momentum')\n",
        "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
        "                    metavar='W', help='weight decay (default: 1e-4)',\n",
        "                    dest='weight_decay')\n",
        "parser.add_argument('-f', '--print-freq', default=1000, type=int,\n",
        "                    metavar='N', help='print frequency (default: 100)')\n",
        "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
        "                    help='path to latest checkpoint (default: none)')\n",
        "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
        "                    help='evaluate model on validation set')\n",
        "parser.add_argument('--seed', default=None, type=int,\n",
        "                    help='seed for initializing training. ')\n",
        "parser.add_argument('--gpu', default=None, type=int,\n",
        "                    help='GPU id to use.')\n",
        "parser.add_argument('--suffix', default='', type=str, metavar='SUFFIX',\n",
        "                    help='add suffix to model save', dest='suffix')\n",
        "parser.add_argument('--save_results', default='validation_results.csv', type=str,\n",
        "                    help='Save validation results in a csv file')\n",
        "parser.add_argument('--ws', '--weighted-sampling', dest='weighted_sampling', action='store_true',\n",
        "                    help='apply weighted random sampling to balance the classes represented in the mini-batch')\n",
        "parser.add_argument('--wl', '--weighted-loss', dest='weighted_loss', action='store_true',\n",
        "                    help='apply weighted loss to balance the classes represented in the mini-batch')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--wl', '--weighted-loss'], dest='weighted_loss', nargs=0, const=True, default=False, type=None, choices=None, help='apply weighted loss to balance the classes represented in the mini-batch', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiOKhD1AJnGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # seed everything to ensure reproducible results from different runs\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        np.random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    ###########################################################################\n",
        "    # Model\n",
        "    ###########################################################################\n",
        "    global best_acc1\n",
        "    # create model\n",
        "    if args.arch == 'LogisticRegression':\n",
        "        model = LogisticRegression(input_size=13, n_classes=args.classes)\n",
        "    elif args.arch == 'NeuralNet':\n",
        "        model = NeuralNet(input_size=13, hidden_size=[32, 16], n_classes=args.classes) #hidden_size=[64, 32]\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        model = model.cuda(args.gpu)\n",
        "\n",
        "    # print(model)\n",
        "    if args.train_file:\n",
        "        print(30 * '=')\n",
        "        print(summary(model, input_size=(1, 13),\n",
        "                      batch_size=args.batch_size, device='cpu'))\n",
        "        print(30 * '=')\n",
        "\n",
        "    ###########################################################################\n",
        "    # save directory\n",
        "    ###########################################################################\n",
        "    save_dir = os.path.join(os.getcwd(), args.save_dir)\n",
        "    save_dir += ('/arch[{}]_optim[{}]_lr[{}]_lrsch[{}]_batch[{}]_'\n",
        "                 'WeightedSampling[{}]').format(args.arch,\n",
        "                                                args.optim,\n",
        "                                                args.lr,\n",
        "                                                args.lr_scheduler,\n",
        "                                                args.batch_size,\n",
        "                                                args.weighted_sampling)\n",
        "    if args.suffix:\n",
        "        save_dir += '_{}'.format(args.suffix)\n",
        "    save_dir = save_dir[:]\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    ###########################################################################\n",
        "    # Criterion and optimizer\n",
        "    ###########################################################################\n",
        "    # Initialise criterion and optimizer\n",
        "    if args.gpu is not None:\n",
        "        criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # define optimizer\n",
        "    print(\"=> using '{}' optimizer\".format(args.optim))\n",
        "    if args.optim == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                    args.lr,\n",
        "                                    momentum=args.momentum,\n",
        "                                    weight_decay=args.weight_decay,\n",
        "                                    nesterov=True)\n",
        "    else:  # default is adam\n",
        "        optimizer = torch.optim.Adam(model.parameters(), args.lr,\n",
        "                                     betas=(0.9, 0.999), eps=1e-08,\n",
        "                                     weight_decay=args.weight_decay,\n",
        "                                     amsgrad=False)\n",
        "\n",
        "    ###########################################################################\n",
        "    # Resume training and load a checkpoint\n",
        "    ###########################################################################\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            if args.gpu is None:\n",
        "                checkpoint = torch.load(args.resume)\n",
        "            else:\n",
        "                # Map model to be loaded to specified single gpu.\n",
        "                loc = 'cuda:{}'.format(args.gpu)\n",
        "                checkpoint = torch.load(args.resume, map_location=loc)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_acc1 = checkpoint['best_acc1']\n",
        "            if args.gpu is not None:\n",
        "                # best_acc1 may be from a checkpoint from a different GPU\n",
        "                best_acc1 = best_acc1.to(args.gpu)\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "\n",
        "    # Data Augmentation\n",
        "    # Learning rate scheduler\n",
        "    print(\"=> using '{}' initial learning rate (lr)\".format(args.lr))\n",
        "    # define learning rate scheduler\n",
        "    scheduler = args.lr_scheduler\n",
        "    if args.lr_scheduler == 'reduce':\n",
        "        print(\"=> using '{}' lr_scheduler\".format(args.lr_scheduler))\n",
        "        # Reduce learning rate when a metric has stopped improving.\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                               mode='min',\n",
        "                                                               factor=0.5,\n",
        "                                                               patience=10)\n",
        "    elif args.lr_scheduler == 'cyclic':\n",
        "        print(\"=> using '{}' lr_scheduler\".format(args.lr_scheduler))\n",
        "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
        "                                                      base_lr=0.00005,\n",
        "                                                      max_lr=0.005)\n",
        "    elif args.lr_scheduler == 'cosine':\n",
        "        print(\"=> using '{}' lr_scheduler\".format(args.lr_scheduler))\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                               T_max=100,\n",
        "                                                               eta_min=0,\n",
        "                                                               last_epoch=-1)\n",
        "    # load train data\n",
        "    if args.train_file:\n",
        "        train_dataset = HeartDiseaseDataset(csv=args.train_file, label_names=LABELS)\n",
        "        if args.weighted_sampling:\n",
        "            train_sampler = torch.utils.data.WeightedRandomSampler(train_dataset.sample_weights,\n",
        "                                                                   len(train_dataset),\n",
        "                                                                   replacement=True)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "\n",
        "        # update criterion\n",
        "        print('class_sample_count ', train_dataset.class_sample_count)\n",
        "        print('class_probability ', train_dataset.class_probability)\n",
        "        print('class_weights ', train_dataset.class_weights)\n",
        "        print('sample_weights ', train_dataset.sample_weights)\n",
        "\n",
        "        if args.weighted_loss:\n",
        "            if args.gpu is not None:\n",
        "                criterion = nn.CrossEntropyLoss(weight=train_dataset.class_weights).cuda(args.gpu)\n",
        "            else:\n",
        "                criterion = nn.CrossEntropyLoss(weight=train_dataset.class_weights)\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                   batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
        "                                                   num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    # load validation data\n",
        "    if args.valid_file:\n",
        "        valid_dataset = HeartDiseaseDataset(csv=args.valid_file, label_names=LABELS)\n",
        "        val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                                 batch_size=args.batch_size, shuffle=False,\n",
        "                                                 num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "        if args.evaluate:\n",
        "            # retrieve correct save path from saved model\n",
        "            save_dir = os.path.split(args.resume)[0]\n",
        "            validate(val_loader, model, criterion, save_dir, args)\n",
        "            return\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # adjust_learning_rate(optimizer, epoch, args)\n",
        "        print_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer,\n",
        "              scheduler, epoch, args)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        acc1 = validate(val_loader, model, criterion, save_dir, args)\n",
        "\n",
        "        # update learning rate based on lr_scheduler\n",
        "        if args.lr_scheduler == 'reduce':\n",
        "            scheduler.step(acc1)\n",
        "        elif args.lr_scheduler == 'cosine':\n",
        "            scheduler.step()\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = acc1 >= best_acc1\n",
        "        best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "        print(\"Saving model [{}]...\".format(save_dir))\n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'arch': args.arch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'best_acc1': best_acc1,\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'criterion': criterion, },\n",
        "                        is_best,\n",
        "                        save_dir=save_dir)\n",
        "        print(30 * '=')\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW4gGx4jKZ9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top2 = AverageMeter('Acc@2', ':6.2f')\n",
        "\n",
        "    progress = ProgressMeter(len(train_loader),\n",
        "                             [batch_time, data_time, losses, top1, top2],\n",
        "                             prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        features, target = batch['features'], batch['target']\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if args.gpu is not None:\n",
        "            features = features.cuda(args.gpu, non_blocking=True)\n",
        "            target = target.cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # compute output\n",
        "        output = model(features)\n",
        "        # compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc2 = accuracy(output, target, topk=(1, 2))\n",
        "        losses.update(loss.item(), features.size(0))\n",
        "        top1.update(acc1[0], features.size(0))\n",
        "        top2.update(acc2[0], features.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update learning rate\n",
        "        if args.lr_scheduler == 'cyclic':\n",
        "            scheduler.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4c_B_KKnU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion, save_dir, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top2 = AverageMeter('Acc@2', ':6.2f')\n",
        "\n",
        "    progress = ProgressMeter(len(val_loader),\n",
        "                             [batch_time, losses, top1, top2],\n",
        "                             prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    # create dataframe to save results in csv file\n",
        "    if args.save_results:\n",
        "        results_df = pd.DataFrame(columns=['target', 'predict', 'predict_top2'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            # print('batch idx{}, batch len {}'.format(i, len(batch)))\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            features, target = batch['features'], batch['target']\n",
        "\n",
        "            if args.gpu is not None:\n",
        "                features = features.cuda(args.gpu, non_blocking=True)\n",
        "                target = target.cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "            # compute output\n",
        "            output = model(features)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Get top2 predictions\n",
        "            _, pred = output.topk(2, 1, True, True)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc2 = accuracy(output, target, topk=(1, 2))\n",
        "            losses.update(loss.item(), features.size(0))\n",
        "            top1.update(acc1[0], features.size(0))\n",
        "            top2.update(acc2[0], features.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # update dataframe with new results\n",
        "            for b in range(len(batch['target'])):\n",
        "                results_df = results_df.append(\n",
        "                    dict(target=batch['target'][b].cpu().numpy(), predict=pred[b, 0].cpu().numpy(),\n",
        "                         predict_top2=pred[b].cpu().numpy()),\n",
        "                    ignore_index=True)\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        # print('params values')\n",
        "        # for name, param in model.named_parameters():\n",
        "        #     if param.requires_grad:\n",
        "        #         print(name, param.data)\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@2 {top2.avg:.3f}'.format(top1=top1, top2=top2))\n",
        "\n",
        "    if args.save_results:\n",
        "        # Save validation results\n",
        "        results_file = os.path.join(save_dir, args.save_results)\n",
        "        results_df.to_csv(results_file, index=False)\n",
        "\n",
        "    return top1.avg"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbVhsTrHKtXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, is_best, save_dir, filename='checkpoint.pth.tar'):\n",
        "    if is_best:\n",
        "        filename = os.path.join(save_dir, 'model_best.pth.tar')\n",
        "    else:\n",
        "        filename = os.path.join(save_dir, filename)\n",
        "\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We3LN18bK8QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDzQfOh7K_1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_KqJn0SLF2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(\"Epoch: [{}] Current learning rate (lr) = {}\".format(\n",
        "            epoch, param_group['lr']))"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}